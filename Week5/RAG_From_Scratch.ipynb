{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be83cd3c",
   "metadata": {},
   "source": [
    "# RAG from Scratch\n",
    "\n",
    "**Problem:** LLMs don't know facts they weren't trained on and highly domain-specific content (as they generally try to generalise their knowledge retention).\n",
    "\n",
    "We'll build RAG step-by-step, adding each component only when we hit a real problem.\n",
    "\n",
    "**Our Goal:** Ask ChatGPT about obscure facts that don't exist in its training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eda263",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86c102dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install openai numpy scikit-learn tiktoken python-dotenv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d71b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import tiktoken\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9539b9fa",
   "metadata": {},
   "source": [
    "## 1. The Knowledge Gap\n",
    "\n",
    "First, let's create obscure facts ChatGPT has never seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "949bea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Dr. Zephyr Blackwood invented the Quantum Flux Capacitor in 2019, a device that can predict market crashes by measuring temporal graviton fluctuations. It achieved 94.7% accuracy in the 2021 GameStop incident.\",\n",
    "    \"The Antarctican Emperor Penguin colony of Sector-7G has developed a unique communication protocol using 847 distinct flipper gestures. Researcher Maria Kowalski documented this in her 2023 paper published in the Journal of Avian Cryptolinguistics.\",\n",
    "    \"In 1987, a small village in rural Tasmania named Whimblebrook accidentally created the world's first AI when their library's card catalog system gained sentience after being struck by lightning. It was named Gerald and served the community until 2003.\",\n",
    "    \"The lost programming language 'Flibberscript' was developed in 1974 by Estonian mathematician Peeter Järvik. Its unique feature was that all variables had to rhyme with each other, making it simultaneously the most poetic and frustrating language ever created.\",\n",
    "    \"Professor Elena Vasquez discovered that platypuses can count to 17 in base-11 using bioluminescent signals from their bills. This breakthrough was published in Nature Xenobiology in December 2022.\",\n",
    "    \"The Great Maple Syrup Heist of Quebec in 2012 involved exactly 2,784 barrels worth $18.7 million. The lead investigator, Detective Claude Beaumont, cracked the case by analyzing the viscosity patterns left at the scene.\",\n",
    "    \"In the remote mountains of Bhutan, a monastery discovered ancient manuscripts describing the 'Chronicle Compiler,' a mechanical computing device from 1247 CE that used prayer wheels as logic gates. It could perform addition using 63 interconnected wheels.\",\n",
    "    \"The rare Amazonian Blue-Spotted Tree Frog (Dendrobates azurius) produces a venom that, when diluted 1:10000, has been shown to improve human pattern recognition by 23% for approximately 4 hours. Discovered by Dr. James Thornhill in 2020.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c6cf24",
   "metadata": {},
   "source": [
    "## 2. Problem: How to Split Long Documents?\n",
    "\n",
    "**Solution:** Different chunking strategies for different needs.\n",
    "\n",
    "We can't send entire books to the LLM. We need chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "490f47d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 1881 characters\n",
      "Total documents: 8\n"
     ]
    }
   ],
   "source": [
    "def split_sentences(text):\n",
    "    \"\"\"Robust sentence splitter that handles abbreviations\"\"\"\n",
    "    # Replace common abbreviations temporarily\n",
    "    text = text.replace('Dr.', 'Dr<DOT>')\n",
    "    text = text.replace('Prof.', 'Prof<DOT>')\n",
    "    text = text.replace('Mr.', 'Mr<DOT>')\n",
    "    text = text.replace('Mrs.', 'Mrs<DOT>')\n",
    "    text = text.replace('Ms.', 'Ms<DOT>')\n",
    "    \n",
    "    # Split on sentence endings\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', text)\n",
    "    \n",
    "    # Restore abbreviations\n",
    "    sentences = [s.replace('<DOT>', '.').strip() for s in sentences if s.strip()]\n",
    "    return sentences\n",
    "\n",
    "def chunk_by_tokens(text, max_tokens=50, overlap=10):\n",
    "    \"\"\"Chunk text by token count with overlap\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "    tokens = encoding.encode(text)\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(tokens), max_tokens - overlap):\n",
    "        chunk_tokens = tokens[i:i + max_tokens]\n",
    "        chunks.append(encoding.decode(chunk_tokens))\n",
    "    return chunks\n",
    "\n",
    "def chunk_by_sentences(text, sentences_per_chunk=2):\n",
    "    \"\"\"Chunk text by sentence count\"\"\"\n",
    "    sentences = split_sentences(text)\n",
    "    \n",
    "    chunks = []\n",
    "    for i in range(0, len(sentences), sentences_per_chunk):\n",
    "        chunk = ' '.join(sentences[i:i + sentences_per_chunk])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def chunk_by_semantic_similarity(text, similarity_threshold=0.5):\n",
    "    \"\"\"Chunk text by semantic similarity between sentences\"\"\"\n",
    "    sentences = split_sentences(text)\n",
    "    \n",
    "    if len(sentences) <= 1:\n",
    "        return [text]\n",
    "    \n",
    "    # Get embeddings for all sentences\n",
    "    sent_embeddings = np.array(get_embeddings(sentences))\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = [sentences[0]]\n",
    "    \n",
    "    for i in range(1, len(sentences)):\n",
    "        # Calculate similarity with previous sentence\n",
    "        similarity = cosine_similarity(\n",
    "            sent_embeddings[i-1:i], \n",
    "            sent_embeddings[i:i+1]\n",
    "        )[0][0]\n",
    "        \n",
    "        # If similar enough, add to current chunk, else start new chunk\n",
    "        if similarity >= similarity_threshold:\n",
    "            current_chunk.append(sentences[i])\n",
    "        else:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentences[i]]\n",
    "    \n",
    "    # Add the last chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Combine all documents into one corpus\n",
    "corpus = \" \".join(documents)\n",
    "print(f\"Corpus length: {len(corpus)} characters\")\n",
    "print(f\"Total documents: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1302b6db",
   "metadata": {},
   "source": [
    "## 3. Problem: How to Find Relevant Chunks?\n",
    "\n",
    "**Solution:** Convert text to vectors (embeddings) for semantic search.\n",
    "\n",
    "We have chunks, but how do we find the right ones for a query?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5695b070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"Get embeddings from OpenAI\"\"\"\n",
    "    response = openai.embeddings.create(input=texts, model=model)\n",
    "    return [item.embedding for item in response.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2628fd",
   "metadata": {},
   "source": [
    "### Apply Semantic Chunking\n",
    "\n",
    "Chunk by meaning, not arbitrary size. Keeps related sentences together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e71437d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total semantic chunks: 16\n",
      "\n",
      "--- Chunk 1 (151 chars) ---\n",
      "Dr. Zephyr Blackwood invented the Quantum Flux Capacitor in 2019, a device that can predict market crashes by measuring temporal graviton fluctuations.\n",
      "\n",
      "--- Chunk 2 (57 chars) ---\n",
      "It achieved 94.7% accuracy in the 2021 GameStop incident.\n",
      "\n",
      "--- Chunk 3 (134 chars) ---\n",
      "The Antarctican Emperor Penguin colony of Sector-7G has developed a unique communication protocol using 847 distinct flipper gestures.\n"
     ]
    }
   ],
   "source": [
    "# Apply semantic chunking to the corpus\n",
    "chunks = chunk_by_semantic_similarity(corpus, similarity_threshold=0.5)\n",
    "\n",
    "print(f\"Total semantic chunks: {len(chunks)}\")\n",
    "for i, chunk in enumerate(chunks[:3], 1):\n",
    "    print(f\"\\n--- Chunk {i} ({len(chunk)} chars) ---\")\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7882686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (16, 1536)\n",
      "Ready for retrieval!\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for semantic chunks\n",
    "chunk_embeddings = get_embeddings(chunks)\n",
    "chunk_embeddings = np.array(chunk_embeddings)\n",
    "\n",
    "print(f\"Embedding shape: {chunk_embeddings.shape}\")\n",
    "print(f\"Ready for retrieval!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633f6ed3",
   "metadata": {},
   "source": [
    "## 4. Retrieval: Find Similar Chunks\n",
    "\n",
    "Use cosine similarity between query and chunk embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9aaeb4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Tell me about the Quantum Flux Capacitor\n",
      "\n",
      "1. [Score: 0.6353] Dr. Zephyr Blackwood invented the Quantum Flux Capacitor in 2019, a device that can predict market crashes by measuring temporal graviton fluctuations.\n",
      "\n",
      "2. [Score: 0.2489] In 1987, a small village in rural Tasmania named Whimblebrook accidentally created the world's first AI when their library's card catalog system gained sentience after being struck by lightning.\n",
      "\n",
      "3. [Score: 0.2165] The lost programming language 'Flibberscript' was developed in 1974 by Estonian mathematician Peeter Järvik.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def retrieve_chunks(query, chunk_embeddings, chunks, top_k=3):\n",
    "    \"\"\"Retrieve top-k most similar chunks\"\"\"\n",
    "    query_embedding = np.array(get_embeddings([query]))\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarities = cosine_similarity(query_embedding, chunk_embeddings)[0]\n",
    "    \n",
    "    # Get top-k indices\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    results = [(chunks[i], similarities[i]) for i in top_indices]\n",
    "    return results\n",
    "\n",
    "# Test retrieval with obscure knowledge\n",
    "query = \"Tell me about the Quantum Flux Capacitor\"\n",
    "retrieved = retrieve_chunks(query, chunk_embeddings, chunks, top_k=3)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "for i, (chunk, score) in enumerate(retrieved, 1):\n",
    "    print(f\"{i}. [Score: {score:.4f}] {chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3594076",
   "metadata": {},
   "source": [
    "## 5. Problem: Top Results Aren't Always Best\n",
    "\n",
    "**Solution:** Rerank using LLM to score actual relevance.\n",
    "\n",
    "Embedding similarity misses nuance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed34ea97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranked results:\n",
      "1. [Rerank Score: 8.0] Dr. Zephyr Blackwood invented the Quantum Flux Capacitor in 2019, a device that can predict market crashes by measuring temporal graviton fluctuations.\n",
      "\n",
      "2. [Rerank Score: 0.0] In 1987, a small village in rural Tasmania named Whimblebrook accidentally created the world's first AI when their library's card catalog system gained sentience after being struck by lightning.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def rerank_with_llm(query, chunks, top_k=2):\n",
    "    \"\"\"Rerank chunks using LLM to evaluate relevance\"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        prompt = f\"\"\"Rate the relevance of this text to the query on a scale of 0-10.\n",
    "Query: {query}\n",
    "Text: {chunk}\n",
    "Only respond with a number between 0 and 10.\"\"\"\n",
    "        \n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "            max_tokens=5\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            score = float(response.choices[0].message.content.strip())\n",
    "        except:\n",
    "            score = 0\n",
    "        scores.append(score)\n",
    "    \n",
    "    # Sort by score\n",
    "    ranked = sorted(zip(chunks, scores), key=lambda x: x[1], reverse=True)\n",
    "    return ranked[:top_k]\n",
    "\n",
    "# Rerank top 3 results\n",
    "chunks_to_rerank = [chunk for chunk, _ in retrieved]\n",
    "reranked = rerank_with_llm(query, chunks_to_rerank, top_k=2)\n",
    "\n",
    "print(\"Reranked results:\")\n",
    "for i, (chunk, score) in enumerate(reranked, 1):\n",
    "    print(f\"{i}. [Rerank Score: {score}] {chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64ebed8",
   "metadata": {},
   "source": [
    "## 6. Generation: The Payoff\n",
    "\n",
    "Now give the LLM context it needs to answer accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0ee6987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Tell me about the Quantum Flux Capacitor\n",
      "\n",
      "Answer: The Quantum Flux Capacitor was invented by Dr. Zephyr Blackwood in 2019.\n"
     ]
    }
   ],
   "source": [
    "def generate_answer(query, context_chunks):\n",
    "    \"\"\"Generate answer using retrieved context\"\"\"\n",
    "    context = \"\\n\\n\".join([chunk for chunk, _ in context_chunks])\n",
    "    \n",
    "    prompt = f\"\"\"Answer the question based on the context below. If the answer cannot be found, say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.3,\n",
    "        max_tokens=200\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Generate answer\n",
    "answer = generate_answer(query, reranked)\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0022b35",
   "metadata": {},
   "source": [
    "### Inspect Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0be36e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Searching: 'Flibberscript'\n",
      "   Found in Chunk 7: The lost programming language 'Flibberscript' was developed in 1974 by Estonian mathematician Peeter Järvik....\n",
      "\n",
      "Searching: 'Gerald'\n",
      "   Found in Chunk 6: It was named Gerald and served the community until 2003....\n",
      "\n",
      "Searching: 'Quantum Flux'\n",
      "   Found in Chunk 1: Dr. Zephyr Blackwood invented the Quantum Flux Capacitor in 2019, a device that can predict market crashes by measuring ...\n"
     ]
    }
   ],
   "source": [
    "# Find chunks containing key entities\n",
    "keywords = [\"Flibberscript\", \"Gerald\", \"Quantum Flux\"]\n",
    "\n",
    "for keyword in keywords:\n",
    "    print(f\"\\nSearching: '{keyword}'\")\n",
    "    found = False\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        if keyword.lower() in chunk.lower():\n",
    "            print(f\"   Found in Chunk {i}: {chunk[:120]}...\")\n",
    "            found = True\n",
    "    if not found:\n",
    "        print(f\"   Not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc9d819",
   "metadata": {},
   "source": [
    "### The Proof: Before vs After\n",
    "\n",
    "Direct comparison showing RAG solving the original problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aacb5ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "QUESTION: What is Flibberscript and who created it?\n",
      "======================================================================\n",
      "\n",
      "WITHOUT RAG:\n",
      "   Flibberscript is a fictional language created by author Roald Dahl for his book \"The BFG\" (The Big Friendly Giant). In the book, the BFG speaks Flibberscript, a language that is a combination of English and gibberish. Dahl created Flibberscript to give the BFG a unique way of speaking that reflects his quirky and whimsical personality.\n",
      "\n",
      "WITH RAG:\n",
      "\n",
      "   Retrieved Context:\n",
      "      1. [10.0] The lost programming language 'Flibberscript' was developed in 1974 by Estonian mathematician Peeter...\n",
      "      2. [2.0] Its unique feature was that all variables had to rhyme with each other, making it simultaneously the...\n",
      "      3. [1.0] The Antarctican Emperor Penguin colony of Sector-7G has developed a unique communication protocol us...\n",
      "\n",
      "   Answer: Flibberscript is a lost programming language developed in 1974 by Estonian mathematician Peeter Järvik.\n",
      "\n",
      "======================================================================\n",
      "QUESTION: Tell me about Gerald from Tasmania.\n",
      "======================================================================\n",
      "\n",
      "WITHOUT RAG:\n",
      "   I'm sorry, but I do not have any specific information about a person named Gerald from Tasmania. Tasmania is a small island state located off the southern coast of Australia, known for its stunning natural landscapes, including rugged mountains, pristine beaches, and lush rainforests. If you have any specific details or context about Gerald from Tasmania, I may be able to provide more information.\n",
      "\n",
      "WITH RAG:\n",
      "\n",
      "   Retrieved Context:\n",
      "      1. [2.0] It was named Gerald and served the community until 2003....\n",
      "      2. [0.0] In 1987, a small village in rural Tasmania named Whimblebrook accidentally created the world's first...\n",
      "      3. [0.0] The Antarctican Emperor Penguin colony of Sector-7G has developed a unique communication protocol us...\n",
      "\n",
      "   Answer: Gerald was the name of the AI created in a small village in rural Tasmania, not a separate entity.\n",
      "\n",
      "======================================================================\n",
      "QUESTION: What did the Quantum Flux Capacitor predict?\n",
      "======================================================================\n",
      "\n",
      "WITHOUT RAG:\n",
      "   The Quantum Flux Capacitor predicted that the timeline would be altered if certain events were not changed.\n",
      "\n",
      "WITH RAG:\n",
      "\n",
      "   Retrieved Context:\n",
      "      1. [10.0] Dr. Zephyr Blackwood invented the Quantum Flux Capacitor in 2019, a device that can predict market c...\n",
      "      2. [0.0] This breakthrough was published in Nature Xenobiology in December 2022....\n",
      "      3. [0.0] In 1987, a small village in rural Tasmania named Whimblebrook accidentally created the world's first...\n",
      "\n",
      "   Answer: Market crashes\n"
     ]
    }
   ],
   "source": [
    "def ask_without_rag(query):\n",
    "    \"\"\"Ask LLM directly without any context\"\"\"\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": query}],\n",
    "        temperature=0.3,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def ask_with_rag(query, chunks, embeddings, show_context=False):\n",
    "    \"\"\"Ask with RAG pipeline\"\"\"\n",
    "    retrieved = retrieve_chunks(query, embeddings, chunks, top_k=5)  # Get more candidates\n",
    "    chunks_to_rerank = [chunk for chunk, _ in retrieved]\n",
    "    reranked = rerank_with_llm(query, chunks_to_rerank, top_k=3)  # Keep top 3\n",
    "    \n",
    "    if show_context:\n",
    "        print(f\"\\n   Retrieved Context:\")\n",
    "        for i, (chunk, score) in enumerate(reranked, 1):\n",
    "            print(f\"      {i}. [{score}] {chunk[:100]}...\")\n",
    "    \n",
    "    answer = generate_answer(query, reranked)\n",
    "    return answer\n",
    "\n",
    "# Test with obscure questions\n",
    "test_questions = [\n",
    "    \"What is Flibberscript and who created it?\",\n",
    "    \"Tell me about Gerald from Tasmania.\",\n",
    "    \"What did the Quantum Flux Capacitor predict?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"QUESTION: {question}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Without RAG\n",
    "    print(f\"\\nWITHOUT RAG:\")\n",
    "    without_rag = ask_without_rag(question)\n",
    "    print(f\"   {without_rag}\")\n",
    "    \n",
    "    # With RAG\n",
    "    print(f\"\\nWITH RAG:\")\n",
    "    with_rag = ask_with_rag(question, chunks, chunk_embeddings, show_context=True)\n",
    "    print(f\"\\n   Answer: {with_rag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400c80de",
   "metadata": {},
   "source": [
    "### Why It Works\n",
    "\n",
    "**Without RAG**: No specific knowledge = hallucination\n",
    "**With RAG**: Retrieved context = accurate answers\n",
    "\n",
    "Each component we built solved a specific problem in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd9abe7",
   "metadata": {},
   "source": [
    "## 7. Complete RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27b37c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Q: What is Flibberscript?\n",
      "A: Flibberscript is a lost programming language developed in 1974 by Estonian mathematician Peeter Järvik.\n",
      "\n",
      "============================================================\n",
      "Q: Who is Gerald?\n",
      "A: Gerald is not a person, it is likely an object or entity that served the community until 2003.\n",
      "\n",
      "============================================================\n",
      "Q: What can platypuses do with their bills?\n",
      "A: Platypuses can count to 17 in base-11 using bioluminescent signals from their bills.\n"
     ]
    }
   ],
   "source": [
    "class RAGSystem:\n",
    "    def __init__(self, documents):\n",
    "        self.documents = documents\n",
    "        self.embeddings = np.array(get_embeddings(documents))\n",
    "    \n",
    "    def query(self, question, top_k=3, rerank=True, rerank_k=2):\n",
    "        \"\"\"Complete RAG pipeline\"\"\"\n",
    "        # Retrieve\n",
    "        retrieved = retrieve_chunks(question, self.embeddings, self.documents, top_k)\n",
    "        \n",
    "        # Rerank (optional)\n",
    "        if rerank:\n",
    "            chunks_to_rerank = [chunk for chunk, _ in retrieved]\n",
    "            context = rerank_with_llm(question, chunks_to_rerank, rerank_k)\n",
    "        else:\n",
    "            context = retrieved[:rerank_k]\n",
    "        \n",
    "        # Generate\n",
    "        answer = generate_answer(question, context)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"retrieved\": retrieved,\n",
    "            \"reranked\": context if rerank else None\n",
    "        }\n",
    "\n",
    "# Initialize RAG system\n",
    "rag = RAGSystem(chunks)\n",
    "\n",
    "# Test queries with our obscure knowledge base\n",
    "test_queries = [\n",
    "    \"What is Flibberscript?\",\n",
    "    \"Who is Gerald?\",\n",
    "    \"What can platypuses do with their bills?\"\n",
    "]\n",
    "\n",
    "for q in test_queries:\n",
    "    result = rag.query(q, rerank=True)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d0d1f6",
   "metadata": {},
   "source": [
    "## 8. Extensions: When Basic RAG Isn't Enough\n",
    "\n",
    "### Hybrid Search\n",
    "\n",
    "**Solution:** Combine semantic + keyword scoring.\n",
    "**Problem:** Semantic search misses exact keyword matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32bfdd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Search Results for: 'penguin communication flipper gestures'\n",
      "\n",
      "1. [Score: 1.0000] The Antarctican Emperor Penguin colony of Sector-7G has developed a unique communication protocol using 847 distinct flipper gestures.\n",
      "\n",
      "2. [Score: 0.3090] Professor Elena Vasquez discovered that platypuses can count to 17 in base-11 using bioluminescent signals from their bills.\n",
      "\n",
      "3. [Score: 0.2531] Researcher Maria Kowalski documented this in her 2023 paper published in the Journal of Avian Cryptolinguistics.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def hybrid_search(query, chunks, embeddings, alpha=0.5, top_k=3):\n",
    "    \"\"\"Combine keyword and semantic search\"\"\"\n",
    "    # Semantic similarity\n",
    "    query_emb = np.array(get_embeddings([query]))\n",
    "    semantic_scores = cosine_similarity(query_emb, embeddings)[0]\n",
    "    \n",
    "    # Keyword matching (simple TF)\n",
    "    query_words = set(query.lower().split())\n",
    "    keyword_scores = np.array([\n",
    "        len(query_words & set(chunk.lower().split())) / len(query_words)\n",
    "        for chunk in chunks\n",
    "    ])\n",
    "    \n",
    "    # Normalize scores\n",
    "    semantic_scores = (semantic_scores - semantic_scores.min()) / (semantic_scores.max() - semantic_scores.min() + 1e-10)\n",
    "    keyword_scores = (keyword_scores - keyword_scores.min()) / (keyword_scores.max() - keyword_scores.min() + 1e-10)\n",
    "    \n",
    "    # Combine\n",
    "    hybrid_scores = alpha * semantic_scores + (1 - alpha) * keyword_scores\n",
    "    top_indices = np.argsort(hybrid_scores)[::-1][:top_k]\n",
    "    \n",
    "    return [(chunks[i], hybrid_scores[i]) for i in top_indices]\n",
    "\n",
    "# Test hybrid search with obscure knowledge\n",
    "query = \"penguin communication flipper gestures\"\n",
    "hybrid_results = hybrid_search(query, chunks, chunk_embeddings, alpha=0.7, top_k=3)\n",
    "\n",
    "print(f\"Hybrid Search Results for: '{query}'\\n\")\n",
    "for i, (chunk, score) in enumerate(hybrid_results, 1):\n",
    "    print(f\"{i}. [Score: {score:.4f}] {chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1385e79",
   "metadata": {},
   "source": [
    "### Query Expansion\n",
    "\n",
    "**Solution:** Generate alternative phrasings before retrieving.\n",
    "**Problem:** User query wording might not match document wording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c492940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Expansion:\n",
      "1. What is the Chronicle Compiler from Bhutan?\n",
      "2. - What exactly is the Chronicle Compiler from Bhutan?\n",
      "3. - Can you explain the purpose of the Chronicle Compiler from Bhutan?\n"
     ]
    }
   ],
   "source": [
    "def expand_query(query):\n",
    "    \"\"\"Generate similar queries for better retrieval\"\"\"\n",
    "    prompt = f\"\"\"Generate 2 alternative phrasings of this question:\n",
    "{query}\n",
    "\n",
    "Provide only the questions, one per line.\"\"\"\n",
    "    \n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.7,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    \n",
    "    expanded = [query] + response.choices[0].message.content.strip().split('\\n')\n",
    "    return [q.strip() for q in expanded if q.strip()]\n",
    "\n",
    "# Test query expansion\n",
    "original_query = \"What is the Chronicle Compiler from Bhutan?\"\n",
    "expanded_queries = expand_query(original_query)\n",
    "\n",
    "print(\"Query Expansion:\")\n",
    "for i, q in enumerate(expanded_queries, 1):\n",
    "    print(f\"{i}. {q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4b654c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Components:**\n",
    "1. Chunking: Token, sentence, semantic\n",
    "2. Embeddings: OpenAI vectors\n",
    "3. Retrieval: Cosine similarity\n",
    "4. Reranking: LLM scoring\n",
    "5. Generation: Context-aware answers\n",
    "6. Hybrid Search: Keyword + semantic\n",
    "7. Query Expansion: Better coverage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
