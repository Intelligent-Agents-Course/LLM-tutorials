{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22fac987",
   "metadata": {},
   "source": [
    "# Pre-training Objectives\n",
    "\n",
    "You have a Transformer architecture. Now what?\n",
    "\n",
    "You can't train it on labeled data - there isn't enough. Supervised datasets have thousands or maybe millions of examples. But language has BILLIONS of sentences freely available on the internet.\n",
    "\n",
    "**The insight:** Create training objectives from raw text itself. Make the model predict parts of text from other parts.\n",
    "\n",
    "This is pre-training. And it's why modern LLMs work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dc8cbd",
   "metadata": {},
   "source": [
    "## The Problem: Supervised Learning Doesn't Scale\n",
    "\n",
    "Traditional NLP:\n",
    "```\n",
    "Labeled data: \"I love cats\" → Positive sentiment\n",
    "              \"This is bad\" → Negative sentiment\n",
    "```\n",
    "\n",
    "You need humans to label thousands of examples for EACH task. Expensive. Slow. Limited.\n",
    "\n",
    "**But:** The internet has trillions of words of unlabeled text. Wikipedia, books, Reddit, news articles.\n",
    "\n",
    "What if we could learn from that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a56200",
   "metadata": {},
   "source": [
    "## Self-Supervised Learning\n",
    "\n",
    "Create labels automatically from the data itself.\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Text: \"The cat sat on the mat\"\n",
    "\n",
    "Task 1: Hide \"cat\", predict it from context\n",
    "\"The [MASK] sat on the mat\" → predict \"cat\"\n",
    "\n",
    "Task 2: Predict next word\n",
    "\"The cat sat on the\" → predict \"mat\"\n",
    "```\n",
    "\n",
    "No humans needed. The text provides its own supervision.\n",
    "\n",
    "This is the foundation of BERT, GPT, and all modern LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a72e5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e08b58",
   "metadata": {},
   "source": [
    "## Causal Language Modeling (CLM)\n",
    "\n",
    "**The task:** Predict the next word given all previous words.\n",
    "\n",
    "```\n",
    "Input:  \"The cat sat on\"\n",
    "Output: \"the\"\n",
    "\n",
    "Input:  \"The cat sat on the\"\n",
    "Output: \"mat\"\n",
    "```\n",
    "\n",
    "This is what GPT does. Train on billions of words, learn to predict what comes next.\n",
    "\n",
    "**Why it works:** To predict the next word well, you need to understand:\n",
    "- Grammar (\"the\" follows \"on\", not \"cat\")\n",
    "- Semantics (\"mat\" is plausible after \"sat on the\")\n",
    "- Context (what \"it\" refers to)\n",
    "- World knowledge (cats sit on mats, not the other way around)\n",
    "\n",
    "All from just predicting next words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90475d6a",
   "metadata": {},
   "source": [
    "### Implementation: Simple Next-Word Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1354e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cat sat on the mat    → [3, 4, 5, 6, 3, 7]\n",
      "the dog ran fast          → [3, 8, 9, 10]\n",
      "the cat ran fast          → [3, 4, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "# Toy vocabulary\n",
    "vocab = {\n",
    "    '<PAD>': 0, '<START>': 1, '<END>': 2,\n",
    "    'the': 3, 'cat': 4, 'sat': 5, 'on': 6, 'mat': 7,\n",
    "    'dog': 8, 'ran': 9, 'fast': 10\n",
    "}\n",
    "idx_to_word = {v: k for k, v in vocab.items()}\n",
    "\n",
    "def tokenize(sentence, vocab):\n",
    "    \"\"\"Convert sentence to token IDs\"\"\"\n",
    "    words = sentence.lower().split()\n",
    "    return [vocab.get(w, 0) for w in words]\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog ran fast\",\n",
    "    \"the cat ran fast\",\n",
    "]\n",
    "\n",
    "for sent in sentences:\n",
    "    tokens = tokenize(sent, vocab)\n",
    "    print(f\"{sent:25s} → {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ec1f22",
   "metadata": {},
   "source": [
    "### Creating Training Examples\n",
    "\n",
    "For each sentence, create multiple (input, target) pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2da56f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples from 'the cat sat on the mat':\n",
      "\n",
      "  Input: the                            → Target: cat\n",
      "  Input: the cat                        → Target: sat\n",
      "  Input: the cat sat                    → Target: on\n",
      "  Input: the cat sat on                 → Target: the\n",
      "  Input: the cat sat on the             → Target: mat\n"
     ]
    }
   ],
   "source": [
    "def create_clm_examples(tokens):\n",
    "    \"\"\"\n",
    "    Create causal LM training examples.\n",
    "    \n",
    "    Input: [w1, w2, w3, w4]\n",
    "    Output:\n",
    "        ([w1], w2)\n",
    "        ([w1, w2], w3)\n",
    "        ([w1, w2, w3], w4)\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    for i in range(1, len(tokens)):\n",
    "        input_seq = tokens[:i]\n",
    "        target = tokens[i]\n",
    "        examples.append((input_seq, target))\n",
    "    return examples\n",
    "\n",
    "# Demo\n",
    "sentence = \"the cat sat on the mat\"\n",
    "tokens = tokenize(sentence, vocab)\n",
    "examples = create_clm_examples(tokens)\n",
    "\n",
    "print(\"Training examples from 'the cat sat on the mat':\\n\")\n",
    "for inp, target in examples:\n",
    "    inp_words = ' '.join([idx_to_word[i] for i in inp])\n",
    "    target_word = idx_to_word[target]\n",
    "    print(f\"  Input: {inp_words:30s} → Target: {target_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84f6313",
   "metadata": {},
   "source": [
    "One sentence gives us 6 training examples. A million sentences? 6 million examples. All free."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5268635",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "Standard cross-entropy loss:\n",
    "\n",
    "```\n",
    "Loss = -log P(w_target | context)\n",
    "```\n",
    "\n",
    "The model outputs a probability distribution over the vocabulary. We want high probability on the correct next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82453b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.675\n",
      "\n",
      "Probability of correct word 'mat': 0.187\n",
      "Top 3 predictions:\n",
      "  mat       : 0.187\n",
      "  fast      : 0.175\n",
      "  cat       : 0.116\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy_loss(logits, target):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: (vocab_size,) unnormalized scores\n",
    "        target: scalar, true token ID\n",
    "    \"\"\"\n",
    "    # Softmax\n",
    "    exp_logits = np.exp(logits - np.max(logits))\n",
    "    probs = exp_logits / np.sum(exp_logits)\n",
    "    \n",
    "    # Negative log likelihood\n",
    "    loss = -np.log(probs[target] + 1e-10)\n",
    "    \n",
    "    return loss, probs\n",
    "\n",
    "# Example\n",
    "vocab_size = len(vocab)\n",
    "logits = np.random.randn(vocab_size)\n",
    "target = vocab['mat']  # True next word is 'mat'\n",
    "\n",
    "loss, probs = cross_entropy_loss(logits, target)\n",
    "print(f\"Loss: {loss:.3f}\")\n",
    "print(f\"\\nProbability of correct word 'mat': {probs[target]:.3f}\")\n",
    "print(f\"Top 3 predictions:\")\n",
    "top3 = np.argsort(probs)[-3:][::-1]\n",
    "for idx in top3:\n",
    "    print(f\"  {idx_to_word[idx]:10s}: {probs[idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eb3cef",
   "metadata": {},
   "source": [
    "## Masked Language Modeling (MLM)\n",
    "\n",
    "**The task:** Hide random words, predict them from context.\n",
    "\n",
    "```\n",
    "Original: \"The cat sat on the mat\"\n",
    "Masked:   \"The [MASK] sat on the mat\"\n",
    "Predict:  \"cat\"\n",
    "```\n",
    "\n",
    "This is what BERT does. Key difference from CLM:\n",
    "\n",
    "**CLM:** Only sees past words (left context)  \n",
    "**MLM:** Sees both past AND future words (bidirectional context)\n",
    "\n",
    "MLM is better for understanding tasks (classification, QA). CLM is better for generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c26816",
   "metadata": {},
   "source": [
    "### BERT's Masking Strategy\n",
    "\n",
    "BERT doesn't just replace with [MASK]. It uses three strategies:\n",
    "\n",
    "**80% of the time:** Replace with [MASK]  \n",
    "```\"The [MASK] sat on the mat\"```\n",
    "\n",
    "**10% of the time:** Replace with random word  \n",
    "```\"The dog sat on the mat\"``` (but still predict \"cat\")\n",
    "\n",
    "**10% of the time:** Keep original  \n",
    "```\"The cat sat on the mat\"``` (still predict \"cat\")\n",
    "\n",
    "**Why?** Prevents the model from only looking at [MASK] tokens. Forces it to learn robust representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a52083d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: the cat sat on the mat\n",
      "Original tokens:   ['the', 'cat', 'sat', 'on', 'the', 'mat']\n",
      "\n",
      "Masked versions:\n",
      "\n",
      "Trial 1: ['the', 'cat', 'sat', 'on', 'the', 'mat']\n",
      "Trial 2: ['<PAD>', 'cat', 'sat', 'on', 'the', 'mat']\n",
      "         → Predict 'the' at position 0\n",
      "Trial 3: ['the', 'cat', 'sat', 'on', '<PAD>', 'mat']\n",
      "         → Predict 'the' at position 4\n",
      "Trial 4: ['the', 'cat', 'sat', 'on', 'on', 'mat']\n",
      "         → Predict 'the' at position 4\n",
      "Trial 5: ['the', '<PAD>', 'sat', 'on', 'the', 'mat']\n",
      "         → Predict 'cat' at position 1\n"
     ]
    }
   ],
   "source": [
    "def create_mlm_example(tokens, vocab, mask_prob=0.15):\n",
    "    \"\"\"\n",
    "    Create masked language model training example.\n",
    "    \n",
    "    Returns:\n",
    "        masked_tokens: input with some tokens masked\n",
    "        targets: positions and true tokens to predict\n",
    "    \"\"\"\n",
    "    masked_tokens = tokens.copy()\n",
    "    targets = []\n",
    "    \n",
    "    MASK_ID = vocab['<PAD>']  # Using PAD as MASK for simplicity\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        # Skip special tokens\n",
    "        if token < 3:\n",
    "            continue\n",
    "            \n",
    "        if np.random.random() < mask_prob:\n",
    "            targets.append((i, token))  # Store position and true token\n",
    "            \n",
    "            rand = np.random.random()\n",
    "            if rand < 0.8:\n",
    "                # 80%: mask\n",
    "                masked_tokens[i] = MASK_ID\n",
    "            elif rand < 0.9:\n",
    "                # 10%: random word\n",
    "                masked_tokens[i] = np.random.randint(3, len(vocab))\n",
    "            # else: 10% keep original\n",
    "    \n",
    "    return masked_tokens, targets\n",
    "\n",
    "# Demo\n",
    "sentence = \"the cat sat on the mat\"\n",
    "tokens = tokenize(sentence, vocab)\n",
    "\n",
    "print(\"Original sentence:\", sentence)\n",
    "print(\"Original tokens:  \", [idx_to_word[t] for t in tokens])\n",
    "print(\"\\nMasked versions:\\n\")\n",
    "\n",
    "for trial in range(5):\n",
    "    masked, targets = create_mlm_example(tokens, vocab)\n",
    "    masked_words = [idx_to_word[t] for t in masked]\n",
    "    print(f\"Trial {trial + 1}: {masked_words}\")\n",
    "    if targets:\n",
    "        for pos, true_token in targets:\n",
    "            print(f\"         → Predict '{idx_to_word[true_token]}' at position {pos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cca4864",
   "metadata": {},
   "source": [
    "## Comparison: CLM vs MLM\n",
    "\n",
    "| Aspect | Causal LM (GPT) | Masked LM (BERT) |\n",
    "|--------|-----------------|------------------|\n",
    "| **Context** | Left-only | Bidirectional |\n",
    "| **Attention** | Masked (causal) | Full (bidirectional) |\n",
    "| **Prediction** | Next token | Masked tokens |\n",
    "| **Training** | Predict all positions | Predict 15% of positions |\n",
    "| **Best for** | Generation | Understanding |\n",
    "| **Examples** | GPT-2, GPT-3, GPT-4 | BERT, RoBERTa |\n",
    "\n",
    "**CLM advantage:** Natural for generation (continue text)  \n",
    "**MLM advantage:** Richer context (sees future) for understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307d7945",
   "metadata": {},
   "source": [
    "## Next Sentence Prediction (NSP)\n",
    "\n",
    "BERT also used a second objective: predict if sentence B follows sentence A.\n",
    "\n",
    "```\n",
    "Sentence A: \"The cat sat on the mat.\"\n",
    "Sentence B: \"It was very comfortable.\"\n",
    "Label: IsNext (1)\n",
    "\n",
    "Sentence A: \"The cat sat on the mat.\"\n",
    "Sentence B: \"The stock market crashed.\"\n",
    "Label: NotNext (0)\n",
    "```\n",
    "\n",
    "**Goal:** Learn sentence relationships.\n",
    "\n",
    "**Reality:** Later research (RoBERTa) found NSP doesn't help much. It's often skipped now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "653d5d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSP training examples:\n",
      "\n",
      "Example 1:\n",
      "  A: It looked very comfortable.\n",
      "  B: The dog ran in the park.\n",
      "  Label: NotNext\n",
      "\n",
      "Example 2:\n",
      "  A: The dog ran in the park.\n",
      "  B: It looked very comfortable.\n",
      "  Label: NotNext\n",
      "\n",
      "Example 3:\n",
      "  A: Birds were singing loudly.\n",
      "  B: The dog ran in the park.\n",
      "  Label: NotNext\n",
      "\n",
      "Example 4:\n",
      "  A: The dog ran in the park.\n",
      "  B: Birds were singing loudly.\n",
      "  Label: NotNext\n",
      "\n",
      "Example 5:\n",
      "  A: It looked very comfortable.\n",
      "  B: The dog ran in the park.\n",
      "  Label: IsNext\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_nsp_example(sentences):\n",
    "    \"\"\"\n",
    "    Create next sentence prediction example.\n",
    "    \n",
    "    50% of time: pick consecutive sentences (label=1)\n",
    "    50% of time: pick random sentences (label=0)\n",
    "    \"\"\"\n",
    "    if np.random.random() < 0.5:\n",
    "        # Positive example: consecutive sentences\n",
    "        idx = np.random.randint(0, len(sentences) - 1)\n",
    "        sent_a = sentences[idx]\n",
    "        sent_b = sentences[idx + 1]\n",
    "        label = 1\n",
    "    else:\n",
    "        # Negative example: random sentences\n",
    "        idx_a, idx_b = np.random.choice(len(sentences), size=2, replace=False)\n",
    "        sent_a = sentences[idx_a]\n",
    "        sent_b = sentences[idx_b]\n",
    "        label = 0\n",
    "    \n",
    "    return sent_a, sent_b, label\n",
    "\n",
    "# Demo\n",
    "corpus = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"It looked very comfortable.\",\n",
    "    \"The dog ran in the park.\",\n",
    "    \"Birds were singing loudly.\",\n",
    "]\n",
    "\n",
    "print(\"NSP training examples:\\n\")\n",
    "for i in range(5):\n",
    "    sent_a, sent_b, label = create_nsp_example(corpus)\n",
    "    label_str = \"IsNext\" if label == 1 else \"NotNext\"\n",
    "    print(f\"Example {i + 1}:\")\n",
    "    print(f\"  A: {sent_a}\")\n",
    "    print(f\"  B: {sent_b}\")\n",
    "    print(f\"  Label: {label_str}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c205f43",
   "metadata": {},
   "source": [
    "## Sequence-to-Sequence Objectives\n",
    "\n",
    "Encoder-decoder models (T5, BART) use denoising objectives:\n",
    "\n",
    "**T5's span corruption:**\n",
    "```\n",
    "Input:  \"The cat <X> on the <Y>\"\n",
    "Output: \"<X> sat <Y> mat\"\n",
    "```\n",
    "\n",
    "Corrupt the input by masking spans, then reconstruct.\n",
    "\n",
    "**BART's noise functions:**\n",
    "- Token masking\n",
    "- Token deletion  \n",
    "- Sentence shuffling\n",
    "- Document rotation\n",
    "\n",
    "The model learns to denoise, which teaches both understanding and generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77c2e920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 span corruption:\n",
      "\n",
      "Trial 1:\n",
      "  Input:  <X0> on the <X1>\n",
      "  Target: <X0> the cat sat <X1> mat\n",
      "\n",
      "Trial 2:\n",
      "  Input:  <X0> <X1> on the <X2>\n",
      "  Target: <X0> the cat <X1> sat <X2> mat\n",
      "\n",
      "Trial 3:\n",
      "  Input:  the cat sat on <X0>\n",
      "  Target: <X0> the mat\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def span_corruption(tokens, vocab, corruption_rate=0.15):\n",
    "    \"\"\"\n",
    "    T5-style span corruption.\n",
    "    Replace random spans with sentinel tokens.\n",
    "    \"\"\"\n",
    "    corrupted = []\n",
    "    targets = []\n",
    "    \n",
    "    i = 0\n",
    "    sentinel_id = 0\n",
    "    \n",
    "    while i < len(tokens):\n",
    "        if np.random.random() < corruption_rate:\n",
    "            # Start a corrupted span\n",
    "            span_length = np.random.randint(1, 4)\n",
    "            span = tokens[i:i + span_length]\n",
    "            \n",
    "            # Add sentinel to input\n",
    "            sentinel = f\"<X{sentinel_id}>\"\n",
    "            corrupted.append(sentinel)\n",
    "            \n",
    "            # Add sentinel and span to target\n",
    "            targets.append(sentinel)\n",
    "            targets.extend([idx_to_word[t] for t in span])\n",
    "            \n",
    "            sentinel_id += 1\n",
    "            i += span_length\n",
    "        else:\n",
    "            corrupted.append(idx_to_word[tokens[i]])\n",
    "            i += 1\n",
    "    \n",
    "    return corrupted, targets\n",
    "\n",
    "# Demo\n",
    "sentence = \"the cat sat on the mat\"\n",
    "tokens = tokenize(sentence, vocab)\n",
    "\n",
    "print(\"T5 span corruption:\\n\")\n",
    "for trial in range(3):\n",
    "    corrupted, targets = span_corruption(tokens, vocab)\n",
    "    print(f\"Trial {trial + 1}:\")\n",
    "    print(f\"  Input:  {' '.join(corrupted)}\")\n",
    "    print(f\"  Target: {' '.join(targets)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f91658c",
   "metadata": {},
   "source": [
    "## Contrastive Learning\n",
    "\n",
    "Another approach: learn by contrasting similar vs dissimilar examples.\n",
    "\n",
    "**SimCLR for text:**\n",
    "- Same sentence, different augmentations → should be similar\n",
    "- Different sentences → should be dissimilar\n",
    "\n",
    "**Sentence embeddings:**\n",
    "```\n",
    "\"The cat sat on the mat\" → [0.2, 0.8, -0.3, ...]\n",
    "\"A feline rested on the rug\" → [0.3, 0.7, -0.2, ...]\n",
    "\"The stock market crashed\" → [-0.5, 0.1, 0.9, ...]\n",
    "```\n",
    "\n",
    "First two should be close in embedding space, third far away."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d683791",
   "metadata": {},
   "source": [
    "## Training at Scale\n",
    "\n",
    "Modern pre-training stats:\n",
    "\n",
    "**BERT (2018):**\n",
    "- 3.3B words (BooksCorpus + Wikipedia)\n",
    "- 4 days on 16 TPUs\n",
    "- 340M parameters\n",
    "\n",
    "**GPT-3 (2020):**\n",
    "- ~500B tokens (Common Crawl, WebText, Books, Wikipedia)\n",
    "- Several thousand petaflop-days\n",
    "- 175B parameters\n",
    "\n",
    "**GPT-4 (2023):**\n",
    "- Unknown but likely >1T tokens\n",
    "- Estimated months on thousands of GPUs\n",
    "- 1.7T parameters (estimated)\n",
    "\n",
    "The trend: more data, bigger models, more compute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5e6f32",
   "metadata": {},
   "source": [
    "## Why Pre-training Works\n",
    "\n",
    "**1. Scale of data**\n",
    "- Billions of tokens vs thousands of labeled examples\n",
    "- Covers diverse domains, styles, topics\n",
    "\n",
    "**2. Rich learning signal**\n",
    "- Every position in every sentence is a training example\n",
    "- Model sees same word in thousands of contexts\n",
    "\n",
    "**3. Transfer learning**\n",
    "- Pre-trained representations work for many downstream tasks\n",
    "- Fine-tune on small labeled dataset → great performance\n",
    "\n",
    "**4. Emergent abilities**\n",
    "- At scale, models learn reasoning, arithmetic, translation\n",
    "- Not explicitly taught - emerges from next-word prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d737f2b8",
   "metadata": {},
   "source": [
    "## Fine-tuning vs Prompting\n",
    "\n",
    "After pre-training, two ways to use the model:\n",
    "\n",
    "**Fine-tuning (BERT era):**\n",
    "```\n",
    "Pre-trained model → Add task-specific head → Train on labeled data\n",
    "```\n",
    "Example: Add classification layer, train on sentiment dataset.\n",
    "\n",
    "**Prompting (GPT-3 era):**\n",
    "```\n",
    "Pre-trained model → Give task instruction as text → Generate answer\n",
    "```\n",
    "Example: \"Classify sentiment: 'I love this movie' →\" → Model generates \"Positive\"\n",
    "\n",
    "Large enough models can do tasks via prompting, no fine-tuning needed. This is IN-CONTEXT LEARNING."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a0d1784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning approach:\n",
      "  1. Take pre-trained BERT\n",
      "  2. Add classification head on top\n",
      "  3. Train on sentiment dataset (1000s examples)\n",
      "  4. Inference: feed text → get label\n",
      "\n",
      "Prompting approach (few-shot):\n",
      "  Prompt:\n",
      "    'I love this movie' → Positive\n",
      "    'This is terrible' → Negative\n",
      "    'Best film ever!' → Positive\n",
      "    'I enjoyed every minute' → ?\n",
      "  \n",
      "  Model generates: 'Positive'\n",
      "  No training needed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Conceptual example of fine-tuning vs prompting\n",
    "\n",
    "def fine_tuning_approach():\n",
    "    \"\"\"\n",
    "    Traditional approach: modify model architecture\n",
    "    \"\"\"\n",
    "    print(\"Fine-tuning approach:\")\n",
    "    print(\"  1. Take pre-trained BERT\")\n",
    "    print(\"  2. Add classification head on top\")\n",
    "    print(\"  3. Train on sentiment dataset (1000s examples)\")\n",
    "    print(\"  4. Inference: feed text → get label\\n\")\n",
    "    \n",
    "def prompting_approach():\n",
    "    \"\"\"\n",
    "    Modern approach: just provide examples in context\n",
    "    \"\"\"\n",
    "    print(\"Prompting approach (few-shot):\")\n",
    "    print(\"  Prompt:\")\n",
    "    print(\"    'I love this movie' → Positive\")\n",
    "    print(\"    'This is terrible' → Negative\")\n",
    "    print(\"    'Best film ever!' → Positive\")\n",
    "    print(\"    'I enjoyed every minute' → ?\")\n",
    "    print(\"  \")\n",
    "    print(\"  Model generates: 'Positive'\")\n",
    "    print(\"  No training needed!\\n\")\n",
    "\n",
    "fine_tuning_approach()\n",
    "prompting_approach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d80f28",
   "metadata": {},
   "source": [
    "## Putting It All Together: Training Pipeline\n",
    "\n",
    "**Step 1: Data collection**\n",
    "- Scrape web, books, Wikipedia\n",
    "- Clean, filter, deduplicate\n",
    "- Billions of tokens\n",
    "\n",
    "**Step 2: Tokenization**\n",
    "- Convert text to subword tokens (BPE, WordPiece)\n",
    "- Build vocabulary (30K-50K tokens typical)\n",
    "\n",
    "**Step 3: Pre-training**\n",
    "- Choose objective (CLM, MLM, etc.)\n",
    "- Train on massive corpus\n",
    "- Days to months on many GPUs/TPUs\n",
    "\n",
    "**Step 4: Evaluation**\n",
    "- Perplexity on held-out data\n",
    "- Downstream task performance\n",
    "\n",
    "**Step 5: Fine-tuning (optional)**\n",
    "- Task-specific training\n",
    "- Or just use prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b228be",
   "metadata": {},
   "source": [
    "## Modern Trends\n",
    "\n",
    "**1. Scaling laws**\n",
    "- Performance improves predictably with more compute, data, parameters\n",
    "- No sign of saturation yet\n",
    "\n",
    "**2. Instruction tuning**\n",
    "- After pre-training, fine-tune on instruction-following tasks\n",
    "- Makes models better at following user requests\n",
    "- Examples: InstructGPT, Flan-T5\n",
    "\n",
    "**3. RLHF (Reinforcement Learning from Human Feedback)**\n",
    "- Train reward model from human preferences\n",
    "- Optimize language model using RL to maximize reward\n",
    "- ChatGPT uses this\n",
    "\n",
    "**4. Sparse models**\n",
    "- Mixture of Experts (MoE): different experts for different inputs\n",
    "- Activate only small subset of parameters per example\n",
    "- Enables larger models with same compute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9db31d",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "**Self-supervised learning:**\n",
    "- Create labels from data itself\n",
    "- Enables training on massive unlabeled corpora\n",
    "- Foundation of modern NLP\n",
    "\n",
    "**Pre-training objectives:**\n",
    "- **CLM:** Predict next word (GPT) - good for generation\n",
    "- **MLM:** Predict masked words (BERT) - good for understanding\n",
    "- **Seq2seq:** Denoise corrupted text (T5, BART) - good for both\n",
    "\n",
    "**The recipe:**\n",
    "1. Collect billions of tokens\n",
    "2. Train with self-supervised objective\n",
    "3. Learn rich representations\n",
    "4. Transfer to downstream tasks\n",
    "\n",
    "**Why it works:**\n",
    "- Scale: more data than any supervised dataset\n",
    "- Generality: learns language, not just specific tasks\n",
    "- Emergence: complex abilities arise from simple objective\n",
    "\n",
    "**The insight:**\n",
    "> \"You shall know a word by the company it keeps.\" - Firth, 1957\n",
    "\n",
    "Modern LLMs take this to the extreme. Learn everything about language just by predicting words in context."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
